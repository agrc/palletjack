<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>palletjack.transform API documentation</title>
<meta name="description" content="Transform pandas dataframes in preparation for loading to AGOL." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>palletjack.transform</code></h1>
</header>
<section id="section-intro">
<p>Transform pandas dataframes in preparation for loading to AGOL.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Transform pandas dataframes in preparation for loading to AGOL.
&#34;&#34;&#34;
import logging
import warnings
from datetime import datetime

import arcgis
import pandas as pd
from arcgis import GeoAccessor, GeoSeriesAccessor

from palletjack import utils

module_logger = logging.getLogger(__name__)


class APIGeocoder:
    &#34;&#34;&#34;Geocode a dataframe using the UGRC Web API Geocoder.

    Instantiate an APIGeocoder object with an api key from developer.mapserv.utah.gov. It will attempt to validate the
    API key. If validation fails, it will raise one of the following errors:

    - RuntimeError: If there was a network or other error
    - ValueError: If the key is invalid
    - UserWarning: If the API responds with some other abnormal result

    The individual geocoding steps are exposed in the `palletjack.utils.Geocoding` class in the utils module for use in
    other settings.
    &#34;&#34;&#34;

    def __init__(self, api_key):
        &#34;&#34;&#34;
        Args:
            api_key (str): API key obtained from developer.mapserv.utah.gov
        &#34;&#34;&#34;
        self.api_key = api_key
        self._class_logger = logging.getLogger(__name__).getChild(self.__class__.__name__)
        utils.Geocoding.validate_api_key(self.api_key)

    def geocode_dataframe(self, dataframe, street_col, zone_col, wkid, rate_limits=(0.015, 0.03), **api_args):
        &#34;&#34;&#34;Geocode a pandas dataframe into a spatially-enabled dataframe

        Addresses that don&#39;t meet the threshold for geocoding (score &gt; 70) are returned as points at 0,0

        Args:
            dataframe (pd.DataFrame): Input data with separate columns for street address and zip or city
            street_col (str): The column containing the street address
            zone_col (str): The column containing either the zip code or the city name
            wkid (int): The projection to return the x/y points in
            rate_limits(Tuple &lt;float&gt;): A lower and upper bound in seconds for pausing between API calls. Defaults to
                (0.015, 0.03)
            **api_args (dict): Keyword arguments to be passed as parameters in the API GET call.

        Returns:
            pd.DataFrame.spatial: Geocoded data as a spatially-enabled DataFrame
        &#34;&#34;&#34;

        start = datetime.now()

        #: Should this return? Should it raise an error instead?
        if dataframe.empty:
            warnings.warn(&#39;No records to geocode (empty dataframe)&#39;, RuntimeWarning)

        dataframe_length = len(dataframe.index)
        reporting_interval = utils.calc_modulus_for_reporting_interval(dataframe_length)
        self._class_logger.info(&#39;Geocoding %s rows...&#39;, dataframe_length)

        street_col_index = dataframe.columns.get_loc(street_col)
        zone_col_index = dataframe.columns.get_loc(zone_col)
        new_rows = []
        for i, row in enumerate(dataframe.itertuples(index=False)):
            if i % reporting_interval == 0:
                self._class_logger.info(&#39;Geocoding row %s of %s, %s%%&#39;, i, dataframe_length, i / dataframe_length * 100)
            row_dict = row._asdict()
            results = utils.Geocoding.geocode_addr(
                row[street_col_index],
                row[zone_col_index],
                self.api_key,
                rate_limits,
                spatialReference=str(wkid),
                **api_args
            )
            self._class_logger.debug(
                &#39;%s of %s: %s, %s = %s&#39;, i, dataframe_length, row[street_col_index], row[zone_col_index], results
            )
            row_dict[&#39;x&#39;], row_dict[&#39;y&#39;], row_dict[&#39;score&#39;], row_dict[&#39;matchAddress&#39;] = results
            new_rows.append(row_dict)

        spatial_dataframe = pd.DataFrame.spatial.from_xy(pd.DataFrame(new_rows), &#39;x&#39;, &#39;y&#39;, sr=int(wkid))

        end = datetime.now()
        self._class_logger.info(&#39;%s Records geocoded in %s&#39;, len(spatial_dataframe.index), (end - start))
        try:
            self._class_logger.debug(&#39;Average time per record: %s&#39;, (end - start) / len(spatial_dataframe.index))
        except ZeroDivisionError:
            warnings.warn(&#39;Empty spatial dataframe after geocoding&#39;, RuntimeWarning)
        return spatial_dataframe


class FeatureServiceMerging:
    &#34;&#34;&#34;Get the live dataframe from a feature service and update it from another dataframe
    &#34;&#34;&#34;

    @staticmethod
    def update_live_data_with_new_data(live_dataframe, new_dataframe, join_column):
        &#34;&#34;&#34;Update a dataframe with data from another

        Args:
            live_dataframe (pd.DataFrame): The dataframe containing info to be updated
            new_dataframe (pd.DataFrame): Dataframe containing source info to use in the update
            join_column (str): The column with unique IDs to be used as a key between the two dataframes

        Raises:
            ValueError: If the join_column is missing from either live or new data
            RuntimeWarning: If there are rows in the new data that are not found in the live data; these will not be
                added to the live dataframe.

        Returns:
            pd.DataFrame: The updated dataframe, with data types converted via .convert_dtypes()
        &#34;&#34;&#34;

        try:
            live_dataframe.set_index(join_column, inplace=True)
            new_dataframe.set_index(join_column, inplace=True)
        except KeyError as error:
            raise ValueError(&#39;Join column not found in live or new dataframes&#39;) from error

        indicator_dataframe = live_dataframe.merge(new_dataframe, on=join_column, how=&#39;outer&#39;, indicator=True)
        new_only_dataframe = indicator_dataframe[indicator_dataframe[&#39;_merge&#39;] == &#39;right_only&#39;]
        if not new_only_dataframe.empty:
            keys_not_found = list(new_only_dataframe.index)
            warnings.warn(
                f&#39;The following keys from the new data were not found in the existing dataset: {keys_not_found}&#39;,
                RuntimeWarning
            )

        live_dataframe.update(new_dataframe)
        return (live_dataframe.reset_index().convert_dtypes())

    @staticmethod
    def get_live_dataframe(gis, feature_service_itemid, layer_index=0):
        &#34;&#34;&#34;Get a spatially-enabled dataframe representation of a hosted feature layer

        Args:
            gis (arcgis.gis.GIS): GIS object of the desired organization
            feature_service_itemid (str): itemid in the gis of the desired hosted feature service
            layer_index (int, optional): Index of the desired layer within the hosted feature service. Defaults to 0.

        Raises:
            RuntimeError: If it fails to load the data

        Returns:
            pd.DataFrame.spatial: Spatially-enabled dataframe representation of the hosted feature layer
        &#34;&#34;&#34;

        try:
            feature_layer = arcgis.features.FeatureLayer.fromitem(
                gis.content.get(feature_service_itemid), layer_id=layer_index
            )
            live_dataframe = feature_layer.query(as_df=True)
        except Exception as error:
            raise RuntimeError(&#39;Failed to load live dataframe&#39;) from error

        return live_dataframe


class DataCleaning:
    &#34;&#34;&#34;Static methods for cleaning dataframes prior to uploading to AGOL
    &#34;&#34;&#34;

    @staticmethod
    def switch_to_nullable_int(dataframe, fields_that_should_be_ints):
        &#34;&#34;&#34;Convert specified fields to panda&#39;s nullable Int64 type to preserve int to EsriFieldTypeInteger mapping

        Args:
            dataframe (pd.DataFrame): Input dataframe with columns to be converted
            fields_that_should_be_ints (list[str]): List of column names to be converted

        Raises:
            TypeError: If any of the conversions fail. Often caused by values that aren&#39;t int-castable floats (ie. x.0)
                or np.nans.

        Returns:
            pd.DataFrame: Input dataframe with columns converted to nullable Int64
        &#34;&#34;&#34;

        int_dict = {field: &#39;Int64&#39; for field in fields_that_should_be_ints}
        try:
            retyped = dataframe.astype(int_dict)
        except TypeError as error:
            raise TypeError(
                &#39;Cannot convert one or more fields to nullable ints. Check for non-int/non-np.nan values.&#39;
            ) from error
        return retyped

    @staticmethod
    def switch_to_float(dataframe, fields_that_should_be_floats):
        &#34;&#34;&#34;Convert specified fields to float, converting empty strings to None first as required

        Args:
            dataframe (pd.DataFrame): Input dataframe with columns to be converted
            fields_that_should_be_floats (list[str]): List of column names to be converted

        Raises:
            TypeError: If any of the conversions fail. Often caused by values that aren&#39;t castable to floats
                (non-empty, non-numeric strings, etc)

        Returns:
            pd.DataFrame: Input dataframe with columns converted to float
        &#34;&#34;&#34;

        float_dict = {field: &#39;float&#39; for field in fields_that_should_be_floats}
        empty_string_dict = {field: {&#39;&#39;: None} for field in fields_that_should_be_floats}
        try:
            no_empty_strings = dataframe.replace(empty_string_dict)
            retyped = no_empty_strings.astype(float_dict)
        except TypeError as error:
            raise TypeError(
                &#39;Cannot convert one or more fields to floats. Check for non-float/non-null values.&#39;
            ) from error
        return retyped

    @staticmethod
    def switch_to_datetime(dataframe, date_fields, **to_datetime_kwargs):
        &#34;&#34;&#34;Convert specified fields to datetime dtypes to ensure proper date formatting for AGOL

        Args:
            dataframe (pd.DataFrame): The source dataframe
            date_fields (List[int]): The fields to convert to datetime
            **to_datetime_kwargs (keyword arguments, optional): Arguments to pass through to pd.to_datetime

        Returns:
            pd.DataFrame: The source dataframe with converted fields.
        &#34;&#34;&#34;

        for field in date_fields:
            dataframe[field] = pd.to_datetime(dataframe[field], **to_datetime_kwargs).dt.tz_localize(None)

        return dataframe

    @staticmethod
    def rename_dataframe_columns_for_agol(dataframe):
        &#34;&#34;&#34;Rename all the columns in a dataframe to valid AGOL column names

        Args:
            dataframe (pd.DataFrame): Dataframe to be renamed

        Returns:
            pd.DataFrame: Input dataframe with renamed columns
        &#34;&#34;&#34;

        rename_dict = utils.rename_columns_for_agol(dataframe.columns)
        renamed_dataframe = dataframe.rename(columns=rename_dict)
        return renamed_dataframe</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="palletjack.transform.APIGeocoder"><code class="flex name class">
<span>class <span class="ident">APIGeocoder</span></span>
<span>(</span><span>api_key)</span>
</code></dt>
<dd>
<div class="desc"><p>Geocode a dataframe using the UGRC Web API Geocoder.</p>
<p>Instantiate an APIGeocoder object with an api key from developer.mapserv.utah.gov. It will attempt to validate the
API key. If validation fails, it will raise one of the following errors:</p>
<ul>
<li>RuntimeError: If there was a network or other error</li>
<li>ValueError: If the key is invalid</li>
<li>UserWarning: If the API responds with some other abnormal result</li>
</ul>
<p>The individual geocoding steps are exposed in the <code><a title="palletjack.utils.Geocoding" href="utils.html#palletjack.utils.Geocoding">Geocoding</a></code> class in the utils module for use in
other settings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>api_key</code></strong> :&ensp;<code>str</code></dt>
<dd>API key obtained from developer.mapserv.utah.gov</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class APIGeocoder:
    &#34;&#34;&#34;Geocode a dataframe using the UGRC Web API Geocoder.

    Instantiate an APIGeocoder object with an api key from developer.mapserv.utah.gov. It will attempt to validate the
    API key. If validation fails, it will raise one of the following errors:

    - RuntimeError: If there was a network or other error
    - ValueError: If the key is invalid
    - UserWarning: If the API responds with some other abnormal result

    The individual geocoding steps are exposed in the `palletjack.utils.Geocoding` class in the utils module for use in
    other settings.
    &#34;&#34;&#34;

    def __init__(self, api_key):
        &#34;&#34;&#34;
        Args:
            api_key (str): API key obtained from developer.mapserv.utah.gov
        &#34;&#34;&#34;
        self.api_key = api_key
        self._class_logger = logging.getLogger(__name__).getChild(self.__class__.__name__)
        utils.Geocoding.validate_api_key(self.api_key)

    def geocode_dataframe(self, dataframe, street_col, zone_col, wkid, rate_limits=(0.015, 0.03), **api_args):
        &#34;&#34;&#34;Geocode a pandas dataframe into a spatially-enabled dataframe

        Addresses that don&#39;t meet the threshold for geocoding (score &gt; 70) are returned as points at 0,0

        Args:
            dataframe (pd.DataFrame): Input data with separate columns for street address and zip or city
            street_col (str): The column containing the street address
            zone_col (str): The column containing either the zip code or the city name
            wkid (int): The projection to return the x/y points in
            rate_limits(Tuple &lt;float&gt;): A lower and upper bound in seconds for pausing between API calls. Defaults to
                (0.015, 0.03)
            **api_args (dict): Keyword arguments to be passed as parameters in the API GET call.

        Returns:
            pd.DataFrame.spatial: Geocoded data as a spatially-enabled DataFrame
        &#34;&#34;&#34;

        start = datetime.now()

        #: Should this return? Should it raise an error instead?
        if dataframe.empty:
            warnings.warn(&#39;No records to geocode (empty dataframe)&#39;, RuntimeWarning)

        dataframe_length = len(dataframe.index)
        reporting_interval = utils.calc_modulus_for_reporting_interval(dataframe_length)
        self._class_logger.info(&#39;Geocoding %s rows...&#39;, dataframe_length)

        street_col_index = dataframe.columns.get_loc(street_col)
        zone_col_index = dataframe.columns.get_loc(zone_col)
        new_rows = []
        for i, row in enumerate(dataframe.itertuples(index=False)):
            if i % reporting_interval == 0:
                self._class_logger.info(&#39;Geocoding row %s of %s, %s%%&#39;, i, dataframe_length, i / dataframe_length * 100)
            row_dict = row._asdict()
            results = utils.Geocoding.geocode_addr(
                row[street_col_index],
                row[zone_col_index],
                self.api_key,
                rate_limits,
                spatialReference=str(wkid),
                **api_args
            )
            self._class_logger.debug(
                &#39;%s of %s: %s, %s = %s&#39;, i, dataframe_length, row[street_col_index], row[zone_col_index], results
            )
            row_dict[&#39;x&#39;], row_dict[&#39;y&#39;], row_dict[&#39;score&#39;], row_dict[&#39;matchAddress&#39;] = results
            new_rows.append(row_dict)

        spatial_dataframe = pd.DataFrame.spatial.from_xy(pd.DataFrame(new_rows), &#39;x&#39;, &#39;y&#39;, sr=int(wkid))

        end = datetime.now()
        self._class_logger.info(&#39;%s Records geocoded in %s&#39;, len(spatial_dataframe.index), (end - start))
        try:
            self._class_logger.debug(&#39;Average time per record: %s&#39;, (end - start) / len(spatial_dataframe.index))
        except ZeroDivisionError:
            warnings.warn(&#39;Empty spatial dataframe after geocoding&#39;, RuntimeWarning)
        return spatial_dataframe</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="palletjack.transform.APIGeocoder.geocode_dataframe"><code class="name flex">
<span>def <span class="ident">geocode_dataframe</span></span>(<span>self, dataframe, street_col, zone_col, wkid, rate_limits=(0.015, 0.03), **api_args)</span>
</code></dt>
<dd>
<div class="desc"><p>Geocode a pandas dataframe into a spatially-enabled dataframe</p>
<p>Addresses that don't meet the threshold for geocoding (score &gt; 70) are returned as points at 0,0</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataframe</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Input data with separate columns for street address and zip or city</dd>
<dt><strong><code>street_col</code></strong> :&ensp;<code>str</code></dt>
<dd>The column containing the street address</dd>
<dt><strong><code>zone_col</code></strong> :&ensp;<code>str</code></dt>
<dd>The column containing either the zip code or the city name</dd>
<dt><strong><code>wkid</code></strong> :&ensp;<code>int</code></dt>
<dd>The projection to return the x/y points in</dd>
<dt>rate_limits(Tuple <float>): A lower and upper bound in seconds for pausing between API calls. Defaults to</dt>
<dt>(0.015, 0.03)</dt>
<dt><strong><code>**api_args</code></strong> :&ensp;<code>dict</code></dt>
<dd>Keyword arguments to be passed as parameters in the API GET call.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame.spatial</code></dt>
<dd>Geocoded data as a spatially-enabled DataFrame</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def geocode_dataframe(self, dataframe, street_col, zone_col, wkid, rate_limits=(0.015, 0.03), **api_args):
    &#34;&#34;&#34;Geocode a pandas dataframe into a spatially-enabled dataframe

    Addresses that don&#39;t meet the threshold for geocoding (score &gt; 70) are returned as points at 0,0

    Args:
        dataframe (pd.DataFrame): Input data with separate columns for street address and zip or city
        street_col (str): The column containing the street address
        zone_col (str): The column containing either the zip code or the city name
        wkid (int): The projection to return the x/y points in
        rate_limits(Tuple &lt;float&gt;): A lower and upper bound in seconds for pausing between API calls. Defaults to
            (0.015, 0.03)
        **api_args (dict): Keyword arguments to be passed as parameters in the API GET call.

    Returns:
        pd.DataFrame.spatial: Geocoded data as a spatially-enabled DataFrame
    &#34;&#34;&#34;

    start = datetime.now()

    #: Should this return? Should it raise an error instead?
    if dataframe.empty:
        warnings.warn(&#39;No records to geocode (empty dataframe)&#39;, RuntimeWarning)

    dataframe_length = len(dataframe.index)
    reporting_interval = utils.calc_modulus_for_reporting_interval(dataframe_length)
    self._class_logger.info(&#39;Geocoding %s rows...&#39;, dataframe_length)

    street_col_index = dataframe.columns.get_loc(street_col)
    zone_col_index = dataframe.columns.get_loc(zone_col)
    new_rows = []
    for i, row in enumerate(dataframe.itertuples(index=False)):
        if i % reporting_interval == 0:
            self._class_logger.info(&#39;Geocoding row %s of %s, %s%%&#39;, i, dataframe_length, i / dataframe_length * 100)
        row_dict = row._asdict()
        results = utils.Geocoding.geocode_addr(
            row[street_col_index],
            row[zone_col_index],
            self.api_key,
            rate_limits,
            spatialReference=str(wkid),
            **api_args
        )
        self._class_logger.debug(
            &#39;%s of %s: %s, %s = %s&#39;, i, dataframe_length, row[street_col_index], row[zone_col_index], results
        )
        row_dict[&#39;x&#39;], row_dict[&#39;y&#39;], row_dict[&#39;score&#39;], row_dict[&#39;matchAddress&#39;] = results
        new_rows.append(row_dict)

    spatial_dataframe = pd.DataFrame.spatial.from_xy(pd.DataFrame(new_rows), &#39;x&#39;, &#39;y&#39;, sr=int(wkid))

    end = datetime.now()
    self._class_logger.info(&#39;%s Records geocoded in %s&#39;, len(spatial_dataframe.index), (end - start))
    try:
        self._class_logger.debug(&#39;Average time per record: %s&#39;, (end - start) / len(spatial_dataframe.index))
    except ZeroDivisionError:
        warnings.warn(&#39;Empty spatial dataframe after geocoding&#39;, RuntimeWarning)
    return spatial_dataframe</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="palletjack.transform.DataCleaning"><code class="flex name class">
<span>class <span class="ident">DataCleaning</span></span>
</code></dt>
<dd>
<div class="desc"><p>Static methods for cleaning dataframes prior to uploading to AGOL</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataCleaning:
    &#34;&#34;&#34;Static methods for cleaning dataframes prior to uploading to AGOL
    &#34;&#34;&#34;

    @staticmethod
    def switch_to_nullable_int(dataframe, fields_that_should_be_ints):
        &#34;&#34;&#34;Convert specified fields to panda&#39;s nullable Int64 type to preserve int to EsriFieldTypeInteger mapping

        Args:
            dataframe (pd.DataFrame): Input dataframe with columns to be converted
            fields_that_should_be_ints (list[str]): List of column names to be converted

        Raises:
            TypeError: If any of the conversions fail. Often caused by values that aren&#39;t int-castable floats (ie. x.0)
                or np.nans.

        Returns:
            pd.DataFrame: Input dataframe with columns converted to nullable Int64
        &#34;&#34;&#34;

        int_dict = {field: &#39;Int64&#39; for field in fields_that_should_be_ints}
        try:
            retyped = dataframe.astype(int_dict)
        except TypeError as error:
            raise TypeError(
                &#39;Cannot convert one or more fields to nullable ints. Check for non-int/non-np.nan values.&#39;
            ) from error
        return retyped

    @staticmethod
    def switch_to_float(dataframe, fields_that_should_be_floats):
        &#34;&#34;&#34;Convert specified fields to float, converting empty strings to None first as required

        Args:
            dataframe (pd.DataFrame): Input dataframe with columns to be converted
            fields_that_should_be_floats (list[str]): List of column names to be converted

        Raises:
            TypeError: If any of the conversions fail. Often caused by values that aren&#39;t castable to floats
                (non-empty, non-numeric strings, etc)

        Returns:
            pd.DataFrame: Input dataframe with columns converted to float
        &#34;&#34;&#34;

        float_dict = {field: &#39;float&#39; for field in fields_that_should_be_floats}
        empty_string_dict = {field: {&#39;&#39;: None} for field in fields_that_should_be_floats}
        try:
            no_empty_strings = dataframe.replace(empty_string_dict)
            retyped = no_empty_strings.astype(float_dict)
        except TypeError as error:
            raise TypeError(
                &#39;Cannot convert one or more fields to floats. Check for non-float/non-null values.&#39;
            ) from error
        return retyped

    @staticmethod
    def switch_to_datetime(dataframe, date_fields, **to_datetime_kwargs):
        &#34;&#34;&#34;Convert specified fields to datetime dtypes to ensure proper date formatting for AGOL

        Args:
            dataframe (pd.DataFrame): The source dataframe
            date_fields (List[int]): The fields to convert to datetime
            **to_datetime_kwargs (keyword arguments, optional): Arguments to pass through to pd.to_datetime

        Returns:
            pd.DataFrame: The source dataframe with converted fields.
        &#34;&#34;&#34;

        for field in date_fields:
            dataframe[field] = pd.to_datetime(dataframe[field], **to_datetime_kwargs).dt.tz_localize(None)

        return dataframe

    @staticmethod
    def rename_dataframe_columns_for_agol(dataframe):
        &#34;&#34;&#34;Rename all the columns in a dataframe to valid AGOL column names

        Args:
            dataframe (pd.DataFrame): Dataframe to be renamed

        Returns:
            pd.DataFrame: Input dataframe with renamed columns
        &#34;&#34;&#34;

        rename_dict = utils.rename_columns_for_agol(dataframe.columns)
        renamed_dataframe = dataframe.rename(columns=rename_dict)
        return renamed_dataframe</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="palletjack.transform.DataCleaning.rename_dataframe_columns_for_agol"><code class="name flex">
<span>def <span class="ident">rename_dataframe_columns_for_agol</span></span>(<span>dataframe)</span>
</code></dt>
<dd>
<div class="desc"><p>Rename all the columns in a dataframe to valid AGOL column names</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataframe</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe to be renamed</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>Input dataframe with renamed columns</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def rename_dataframe_columns_for_agol(dataframe):
    &#34;&#34;&#34;Rename all the columns in a dataframe to valid AGOL column names

    Args:
        dataframe (pd.DataFrame): Dataframe to be renamed

    Returns:
        pd.DataFrame: Input dataframe with renamed columns
    &#34;&#34;&#34;

    rename_dict = utils.rename_columns_for_agol(dataframe.columns)
    renamed_dataframe = dataframe.rename(columns=rename_dict)
    return renamed_dataframe</code></pre>
</details>
</dd>
<dt id="palletjack.transform.DataCleaning.switch_to_datetime"><code class="name flex">
<span>def <span class="ident">switch_to_datetime</span></span>(<span>dataframe, date_fields, **to_datetime_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert specified fields to datetime dtypes to ensure proper date formatting for AGOL</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataframe</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The source dataframe</dd>
<dt><strong><code>date_fields</code></strong> :&ensp;<code>List[int]</code></dt>
<dd>The fields to convert to datetime</dd>
<dt><strong><code>**to_datetime_kwargs</code></strong> :&ensp;<code>keyword arguments</code>, optional</dt>
<dd>Arguments to pass through to pd.to_datetime</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>The source dataframe with converted fields.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def switch_to_datetime(dataframe, date_fields, **to_datetime_kwargs):
    &#34;&#34;&#34;Convert specified fields to datetime dtypes to ensure proper date formatting for AGOL

    Args:
        dataframe (pd.DataFrame): The source dataframe
        date_fields (List[int]): The fields to convert to datetime
        **to_datetime_kwargs (keyword arguments, optional): Arguments to pass through to pd.to_datetime

    Returns:
        pd.DataFrame: The source dataframe with converted fields.
    &#34;&#34;&#34;

    for field in date_fields:
        dataframe[field] = pd.to_datetime(dataframe[field], **to_datetime_kwargs).dt.tz_localize(None)

    return dataframe</code></pre>
</details>
</dd>
<dt id="palletjack.transform.DataCleaning.switch_to_float"><code class="name flex">
<span>def <span class="ident">switch_to_float</span></span>(<span>dataframe, fields_that_should_be_floats)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert specified fields to float, converting empty strings to None first as required</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataframe</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Input dataframe with columns to be converted</dd>
<dt><strong><code>fields_that_should_be_floats</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>List of column names to be converted</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If any of the conversions fail. Often caused by values that aren't castable to floats
(non-empty, non-numeric strings, etc)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>Input dataframe with columns converted to float</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def switch_to_float(dataframe, fields_that_should_be_floats):
    &#34;&#34;&#34;Convert specified fields to float, converting empty strings to None first as required

    Args:
        dataframe (pd.DataFrame): Input dataframe with columns to be converted
        fields_that_should_be_floats (list[str]): List of column names to be converted

    Raises:
        TypeError: If any of the conversions fail. Often caused by values that aren&#39;t castable to floats
            (non-empty, non-numeric strings, etc)

    Returns:
        pd.DataFrame: Input dataframe with columns converted to float
    &#34;&#34;&#34;

    float_dict = {field: &#39;float&#39; for field in fields_that_should_be_floats}
    empty_string_dict = {field: {&#39;&#39;: None} for field in fields_that_should_be_floats}
    try:
        no_empty_strings = dataframe.replace(empty_string_dict)
        retyped = no_empty_strings.astype(float_dict)
    except TypeError as error:
        raise TypeError(
            &#39;Cannot convert one or more fields to floats. Check for non-float/non-null values.&#39;
        ) from error
    return retyped</code></pre>
</details>
</dd>
<dt id="palletjack.transform.DataCleaning.switch_to_nullable_int"><code class="name flex">
<span>def <span class="ident">switch_to_nullable_int</span></span>(<span>dataframe, fields_that_should_be_ints)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert specified fields to panda's nullable Int64 type to preserve int to EsriFieldTypeInteger mapping</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataframe</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Input dataframe with columns to be converted</dd>
<dt><strong><code>fields_that_should_be_ints</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>List of column names to be converted</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If any of the conversions fail. Often caused by values that aren't int-castable floats (ie. x.0)
or np.nans.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>Input dataframe with columns converted to nullable Int64</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def switch_to_nullable_int(dataframe, fields_that_should_be_ints):
    &#34;&#34;&#34;Convert specified fields to panda&#39;s nullable Int64 type to preserve int to EsriFieldTypeInteger mapping

    Args:
        dataframe (pd.DataFrame): Input dataframe with columns to be converted
        fields_that_should_be_ints (list[str]): List of column names to be converted

    Raises:
        TypeError: If any of the conversions fail. Often caused by values that aren&#39;t int-castable floats (ie. x.0)
            or np.nans.

    Returns:
        pd.DataFrame: Input dataframe with columns converted to nullable Int64
    &#34;&#34;&#34;

    int_dict = {field: &#39;Int64&#39; for field in fields_that_should_be_ints}
    try:
        retyped = dataframe.astype(int_dict)
    except TypeError as error:
        raise TypeError(
            &#39;Cannot convert one or more fields to nullable ints. Check for non-int/non-np.nan values.&#39;
        ) from error
    return retyped</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="palletjack.transform.FeatureServiceMerging"><code class="flex name class">
<span>class <span class="ident">FeatureServiceMerging</span></span>
</code></dt>
<dd>
<div class="desc"><p>Get the live dataframe from a feature service and update it from another dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureServiceMerging:
    &#34;&#34;&#34;Get the live dataframe from a feature service and update it from another dataframe
    &#34;&#34;&#34;

    @staticmethod
    def update_live_data_with_new_data(live_dataframe, new_dataframe, join_column):
        &#34;&#34;&#34;Update a dataframe with data from another

        Args:
            live_dataframe (pd.DataFrame): The dataframe containing info to be updated
            new_dataframe (pd.DataFrame): Dataframe containing source info to use in the update
            join_column (str): The column with unique IDs to be used as a key between the two dataframes

        Raises:
            ValueError: If the join_column is missing from either live or new data
            RuntimeWarning: If there are rows in the new data that are not found in the live data; these will not be
                added to the live dataframe.

        Returns:
            pd.DataFrame: The updated dataframe, with data types converted via .convert_dtypes()
        &#34;&#34;&#34;

        try:
            live_dataframe.set_index(join_column, inplace=True)
            new_dataframe.set_index(join_column, inplace=True)
        except KeyError as error:
            raise ValueError(&#39;Join column not found in live or new dataframes&#39;) from error

        indicator_dataframe = live_dataframe.merge(new_dataframe, on=join_column, how=&#39;outer&#39;, indicator=True)
        new_only_dataframe = indicator_dataframe[indicator_dataframe[&#39;_merge&#39;] == &#39;right_only&#39;]
        if not new_only_dataframe.empty:
            keys_not_found = list(new_only_dataframe.index)
            warnings.warn(
                f&#39;The following keys from the new data were not found in the existing dataset: {keys_not_found}&#39;,
                RuntimeWarning
            )

        live_dataframe.update(new_dataframe)
        return (live_dataframe.reset_index().convert_dtypes())

    @staticmethod
    def get_live_dataframe(gis, feature_service_itemid, layer_index=0):
        &#34;&#34;&#34;Get a spatially-enabled dataframe representation of a hosted feature layer

        Args:
            gis (arcgis.gis.GIS): GIS object of the desired organization
            feature_service_itemid (str): itemid in the gis of the desired hosted feature service
            layer_index (int, optional): Index of the desired layer within the hosted feature service. Defaults to 0.

        Raises:
            RuntimeError: If it fails to load the data

        Returns:
            pd.DataFrame.spatial: Spatially-enabled dataframe representation of the hosted feature layer
        &#34;&#34;&#34;

        try:
            feature_layer = arcgis.features.FeatureLayer.fromitem(
                gis.content.get(feature_service_itemid), layer_id=layer_index
            )
            live_dataframe = feature_layer.query(as_df=True)
        except Exception as error:
            raise RuntimeError(&#39;Failed to load live dataframe&#39;) from error

        return live_dataframe</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="palletjack.transform.FeatureServiceMerging.get_live_dataframe"><code class="name flex">
<span>def <span class="ident">get_live_dataframe</span></span>(<span>gis, feature_service_itemid, layer_index=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a spatially-enabled dataframe representation of a hosted feature layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gis</code></strong> :&ensp;<code>arcgis.gis.GIS</code></dt>
<dd>GIS object of the desired organization</dd>
<dt><strong><code>feature_service_itemid</code></strong> :&ensp;<code>str</code></dt>
<dd>itemid in the gis of the desired hosted feature service</dd>
<dt><strong><code>layer_index</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Index of the desired layer within the hosted feature service. Defaults to 0.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If it fails to load the data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame.spatial</code></dt>
<dd>Spatially-enabled dataframe representation of the hosted feature layer</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_live_dataframe(gis, feature_service_itemid, layer_index=0):
    &#34;&#34;&#34;Get a spatially-enabled dataframe representation of a hosted feature layer

    Args:
        gis (arcgis.gis.GIS): GIS object of the desired organization
        feature_service_itemid (str): itemid in the gis of the desired hosted feature service
        layer_index (int, optional): Index of the desired layer within the hosted feature service. Defaults to 0.

    Raises:
        RuntimeError: If it fails to load the data

    Returns:
        pd.DataFrame.spatial: Spatially-enabled dataframe representation of the hosted feature layer
    &#34;&#34;&#34;

    try:
        feature_layer = arcgis.features.FeatureLayer.fromitem(
            gis.content.get(feature_service_itemid), layer_id=layer_index
        )
        live_dataframe = feature_layer.query(as_df=True)
    except Exception as error:
        raise RuntimeError(&#39;Failed to load live dataframe&#39;) from error

    return live_dataframe</code></pre>
</details>
</dd>
<dt id="palletjack.transform.FeatureServiceMerging.update_live_data_with_new_data"><code class="name flex">
<span>def <span class="ident">update_live_data_with_new_data</span></span>(<span>live_dataframe, new_dataframe, join_column)</span>
</code></dt>
<dd>
<div class="desc"><p>Update a dataframe with data from another</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>live_dataframe</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The dataframe containing info to be updated</dd>
<dt><strong><code>new_dataframe</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe containing source info to use in the update</dd>
<dt><strong><code>join_column</code></strong> :&ensp;<code>str</code></dt>
<dd>The column with unique IDs to be used as a key between the two dataframes</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the join_column is missing from either live or new data</dd>
<dt><code>RuntimeWarning</code></dt>
<dd>If there are rows in the new data that are not found in the live data; these will not be
added to the live dataframe.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>The updated dataframe, with data types converted via .convert_dtypes()</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def update_live_data_with_new_data(live_dataframe, new_dataframe, join_column):
    &#34;&#34;&#34;Update a dataframe with data from another

    Args:
        live_dataframe (pd.DataFrame): The dataframe containing info to be updated
        new_dataframe (pd.DataFrame): Dataframe containing source info to use in the update
        join_column (str): The column with unique IDs to be used as a key between the two dataframes

    Raises:
        ValueError: If the join_column is missing from either live or new data
        RuntimeWarning: If there are rows in the new data that are not found in the live data; these will not be
            added to the live dataframe.

    Returns:
        pd.DataFrame: The updated dataframe, with data types converted via .convert_dtypes()
    &#34;&#34;&#34;

    try:
        live_dataframe.set_index(join_column, inplace=True)
        new_dataframe.set_index(join_column, inplace=True)
    except KeyError as error:
        raise ValueError(&#39;Join column not found in live or new dataframes&#39;) from error

    indicator_dataframe = live_dataframe.merge(new_dataframe, on=join_column, how=&#39;outer&#39;, indicator=True)
    new_only_dataframe = indicator_dataframe[indicator_dataframe[&#39;_merge&#39;] == &#39;right_only&#39;]
    if not new_only_dataframe.empty:
        keys_not_found = list(new_only_dataframe.index)
        warnings.warn(
            f&#39;The following keys from the new data were not found in the existing dataset: {keys_not_found}&#39;,
            RuntimeWarning
        )

    live_dataframe.update(new_dataframe)
    return (live_dataframe.reset_index().convert_dtypes())</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="palletjack" href="index.html">palletjack</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="palletjack.transform.APIGeocoder" href="#palletjack.transform.APIGeocoder">APIGeocoder</a></code></h4>
<ul class="">
<li><code><a title="palletjack.transform.APIGeocoder.geocode_dataframe" href="#palletjack.transform.APIGeocoder.geocode_dataframe">geocode_dataframe</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="palletjack.transform.DataCleaning" href="#palletjack.transform.DataCleaning">DataCleaning</a></code></h4>
<ul class="">
<li><code><a title="palletjack.transform.DataCleaning.rename_dataframe_columns_for_agol" href="#palletjack.transform.DataCleaning.rename_dataframe_columns_for_agol">rename_dataframe_columns_for_agol</a></code></li>
<li><code><a title="palletjack.transform.DataCleaning.switch_to_datetime" href="#palletjack.transform.DataCleaning.switch_to_datetime">switch_to_datetime</a></code></li>
<li><code><a title="palletjack.transform.DataCleaning.switch_to_float" href="#palletjack.transform.DataCleaning.switch_to_float">switch_to_float</a></code></li>
<li><code><a title="palletjack.transform.DataCleaning.switch_to_nullable_int" href="#palletjack.transform.DataCleaning.switch_to_nullable_int">switch_to_nullable_int</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="palletjack.transform.FeatureServiceMerging" href="#palletjack.transform.FeatureServiceMerging">FeatureServiceMerging</a></code></h4>
<ul class="">
<li><code><a title="palletjack.transform.FeatureServiceMerging.get_live_dataframe" href="#palletjack.transform.FeatureServiceMerging.get_live_dataframe">get_live_dataframe</a></code></li>
<li><code><a title="palletjack.transform.FeatureServiceMerging.update_live_data_with_new_data" href="#palletjack.transform.FeatureServiceMerging.update_live_data_with_new_data">update_live_data_with_new_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>